{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbors is a classification algorithm familiar to many. It takes a unlabled observation (the star above) and compares it to a population of labled observations (blue and orange circles). By finding the K nearest circles to the star, we can infer the class label for the star through majority voting.\n",
    "\n",
    "We can also use KNN in the context of time series data. The question becomes, \"how do we compute the distance between two timeseries sequences\"? \"Dynamic Time Warping\" is a technique that was heavily used for speech recognition in the 80s. The DTW algorithm finds the optimum alignment between two sequences of observations by warping the time dimension with certain constraints.\n",
    "\n",
    "Because of this temporal dimension warping, DTW is good for classifying sequences that have different frequences or that are out of phase. Later we'll see how we can use DTW to classify whether a person is walking, lying down, sitting etc.\n",
    "\n",
    "The above right diagram shows how a DTW distance matrix representation. Each cell is computed by measuring the distance between $A_i$ and $B_j$. The red path represents the shortest path and hence optimum alignment of the two sequences. \n",
    "\n",
    "$$DTW_{AB} = SUM(shortest\\ paths_{AB})$$\n",
    "\n",
    "####Max Warping Window Allowed\n",
    "Computing the full distance matrix between A and B scales with $O(n^2)$, where n is the number of sequences in A and B. This performance can be improved by constraining the amount of warping allowed. This limits the number of cells that need to be computed in the DTW distance matrix. Research by Keogh et al has shown that a warping window not only improves performance but also improves classification accuracy [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Coded in Python\n",
    "The below python code represents the KNN & DTW classification algorithm. The following key methods are described:\n",
    "\n",
    "| Method        | Description                                                                                                           |\n",
    "|---------------|-----------------------------------------------------------------------------------------------------------------------|\n",
    "| `_dtw_distance` | computes the Dynamic Time Warping distance between two sequences                                                      |\n",
    "| `_dist_matrix`  | computes the distance matrix between $A$ and $B$                  |\n",
    "| `predict`       | uses both of the above methods to compute the class labels and probability of dataset $B$ through K Nearest Neighbors |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastdtw in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.4)\n",
      "Collecting tslearn\n",
      "  Downloading tslearn-0.6.2-py3-none-any.whl (369 kB)\n",
      "                                              0.0/369.8 kB ? eta -:--:--\n",
      "     -----------------------------          286.7/369.8 kB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 369.8/369.8 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastdtw) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tslearn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tslearn) (1.3.1)\n",
      "Collecting numba (from tslearn)\n",
      "  Downloading numba-0.58.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     ------                                   0.4/2.6 MB 12.5 MB/s eta 0:00:01\n",
      "     -----------                              0.7/2.6 MB 9.5 MB/s eta 0:00:01\n",
      "     -----------------                        1.2/2.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------                    1.4/2.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------              1.8/2.6 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------        2.2/2.6 MB 8.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   2.5/2.6 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tslearn) (1.3.2)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba->tslearn)\n",
      "  Downloading llvmlite-0.41.0-cp311-cp311-win_amd64.whl (28.1 MB)\n",
      "                                              0.0/28.1 MB ? eta -:--:--\n",
      "                                              0.5/28.1 MB 16.5 MB/s eta 0:00:02\n",
      "     -                                        0.9/28.1 MB 9.2 MB/s eta 0:00:03\n",
      "     -                                        1.2/28.1 MB 8.5 MB/s eta 0:00:04\n",
      "     --                                       1.6/28.1 MB 9.0 MB/s eta 0:00:03\n",
      "     --                                       1.9/28.1 MB 8.1 MB/s eta 0:00:04\n",
      "     ---                                      2.2/28.1 MB 8.4 MB/s eta 0:00:04\n",
      "     ---                                      2.6/28.1 MB 7.9 MB/s eta 0:00:04\n",
      "     ----                                     2.9/28.1 MB 7.8 MB/s eta 0:00:04\n",
      "     ----                                     3.3/28.1 MB 8.1 MB/s eta 0:00:04\n",
      "     -----                                    3.6/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "     -----                                    4.0/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "     ------                                   4.3/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "     ------                                   4.7/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "     -------                                  5.0/28.1 MB 7.6 MB/s eta 0:00:04\n",
      "     -------                                  5.4/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     --------                                 5.7/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     --------                                 6.1/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ---------                                6.4/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ---------                                6.8/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ----------                               7.1/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ----------                               7.5/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     -----------                              7.8/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     -----------                              8.2/28.1 MB 7.6 MB/s eta 0:00:03\n",
      "     ------------                             8.5/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     ------------                             8.9/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     -------------                            9.2/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     -------------                            9.6/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     --------------                           9.9/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     --------------                           10.3/28.1 MB 7.5 MB/s eta 0:00:03\n",
      "     ---------------                          10.6/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     ---------------                          11.0/28.1 MB 7.3 MB/s eta 0:00:03\n",
      "     ----------------                         11.3/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     ----------------                         11.7/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     -----------------                        12.0/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     -----------------                        12.4/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     ------------------                       12.7/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     ------------------                       13.1/28.1 MB 7.4 MB/s eta 0:00:03\n",
      "     -------------------                      13.4/28.1 MB 7.3 MB/s eta 0:00:03\n",
      "     -------------------                      13.8/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     --------------------                     14.1/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     --------------------                     14.5/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------------------                    14.8/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     ---------------------                    15.1/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ----------------------                   15.5/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ----------------------                   15.9/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     -----------------------                  16.2/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     -----------------------                  16.6/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ------------------------                 16.9/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ------------------------                 17.2/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     -------------------------                17.6/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     -------------------------                18.0/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     --------------------------               18.3/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     --------------------------               18.7/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------------------------              19.0/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------------------------              19.4/28.1 MB 7.4 MB/s eta 0:00:02\n",
      "     ---------------------------              19.6/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     ----------------------------             20.0/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     ----------------------------             20.3/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     -----------------------------            20.7/28.1 MB 7.3 MB/s eta 0:00:02\n",
      "     -----------------------------            21.0/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------           21.3/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------           21.7/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------          22.0/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------          22.4/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------         22.7/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------         23.1/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------        23.4/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------        23.8/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------------       24.1/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------------       24.5/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -----------------------------------      24.8/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -----------------------------------      25.2/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------     25.5/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------------     25.9/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------    26.2/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------    26.6/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   26.9/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   27.3/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  27.6/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  28.0/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  28.1/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 28.1/28.1 MB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anita singh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->tslearn) (3.2.0)\n",
      "Installing collected packages: llvmlite, numba, tslearn\n",
      "Successfully installed llvmlite-0.41.0 numba-0.58.0 tslearn-0.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install fastdtw tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_elec = pd.read_csv(\"../data/KOL_15mins_final_merged_numeric.csv\")\n",
    "df_temp=df_elec[\"idmeter\"].unique()\n",
    "print(df_temp.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088, 10088]\n"
     ]
    }
   ],
   "source": [
    "df_batch_elec = [df_elec[df_elec[\"idmeter\"]==x].iloc[0:104,1:] for x in df_elec[\"idmeter\"].unique()]\n",
    "df_elec_size_map = [x.size for x in df_batch_elec]\n",
    "print(df_elec_size_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TimeSeriesKMeans(max_iter=10, metric=&#x27;dtw&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TimeSeriesKMeans</label><div class=\"sk-toggleable__content\"><pre>TimeSeriesKMeans(max_iter=10, metric=&#x27;dtw&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TimeSeriesKMeans(max_iter=10, metric='dtw')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "model = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", max_iter=10)\n",
    "data = []\n",
    "i=0\n",
    "for x in df_batch_elec:\n",
    "    data.append([[x.iloc[1+i,1]] for i in range(96)])\n",
    "    i=i+1\n",
    "#print(data)\n",
    "model.fit(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_clusters0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m X_train \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     18\u001b[0m numpy\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(X_train)\n\u001b[1;32m---> 19\u001b[0m X_train \u001b[38;5;241m=\u001b[39m TimeSeriesScalerMeanVariance()\u001b[38;5;241m.\u001b[39mfit_transform(X_train[:\u001b[43mnum_clusters0\u001b[49m])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print(X_train)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make time series shorter\u001b[39;00m\n\u001b[0;32m     22\u001b[0m X_train \u001b[38;5;241m=\u001b[39m TimeSeriesResampler(sz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_clusters0' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "\n",
    "# X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
    "# X_train = X_train[y_train < 4]  # Keep first 3 classes\n",
    "\n",
    "\n",
    "\n",
    "# Keep only 50 time series\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "X_train = data\n",
    "numpy.random.shuffle(X_train)\n",
    "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train[:100])\n",
    "# print(X_train)\n",
    "# Make time series shorter\n",
    "X_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\n",
    "sz = X_train.shape[1]\n",
    "\n",
    "# Euclidean k-means\n",
    "print(\"Euclidean k-means\")\n",
    "num_clusters = 2\n",
    "km = TimeSeriesKMeans(n_clusters=num_clusters, verbose=True, random_state=seed)\n",
    "y_pred = km.fit_predict(X_train)\n",
    "\n",
    "plt.figure()\n",
    "for yi in range(num_clusters):\n",
    "    plt.subplot(num_clusters, 3, 3*yi + 1)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"Euclidean $k$-means\")\n",
    "\n",
    "# DBA-k-means\n",
    "print(\"DBA k-means\")\n",
    "dba_km = TimeSeriesKMeans(n_clusters=num_clusters,\n",
    "                          n_init=2,\n",
    "                          metric=\"dtw\",\n",
    "                          verbose=True,\n",
    "                          max_iter_barycenter=num_clusters,\n",
    "                          random_state=seed)\n",
    "y_pred = dba_km.fit_predict(X_train)\n",
    "\n",
    "for yi in range(num_clusters):\n",
    "    plt.subplot(num_clusters, 3, 3*yi+2)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(dba_km.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"DBA $k$-means\")\n",
    "\n",
    "# Soft-DTW-k-means\n",
    "print(\"Soft-DTW k-means\")\n",
    "sdtw_km = TimeSeriesKMeans(n_clusters=num_clusters,\n",
    "                           metric=\"softdtw\",\n",
    "                           metric_params={\"gamma\": .01},\n",
    "                           verbose=True,\n",
    "                           random_state=seed)\n",
    "y_pred = sdtw_km.fit_predict(X_train)\n",
    "\n",
    "for yi in range(num_clusters):\n",
    "    plt.subplot(num_clusters, 3, 3*yi+3)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"Soft-DTW $k$-means\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "x = np.array([[1,1], [2,2], [3,3], [4,4], [5,5]])\n",
    "y = np.array([[2,2], [3,3], [4,4]])\n",
    "distance, path = fastdtw(x, y, dist=euclidean)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class KnnDtw(object):\n",
    "    \"\"\"K-nearest neighbor classifier using dynamic time warping\n",
    "    as the distance measure between pairs of time series arrays\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neighbors : int, optional (default = 5)\n",
    "        Number of neighbors to use by default for KNN\n",
    "        \n",
    "    max_warping_window : int, optional (default = infinity)\n",
    "        Maximum warping window allowed by the DTW dynamic\n",
    "        programming function\n",
    "            \n",
    "    subsample_step : int, optional (default = 1)\n",
    "        Step size for the timeseries array. By setting subsample_step = 2,\n",
    "        the timeseries length will be reduced by 50% because every second\n",
    "        item is skipped. Implemented by x[:, ::subsample_step]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors=5, max_warping_window=10000, subsample_step=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_warping_window = max_warping_window\n",
    "        self.subsample_step = subsample_step\n",
    "    \n",
    "    def fit(self, x, l):\n",
    "        \"\"\"Fit the model using x as training data and l as class labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "            Training data set for input into KNN classifer\n",
    "            \n",
    "        l : array of shape [n_samples]\n",
    "            Training labels for input into KNN classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.l = l\n",
    "        \n",
    "    def _dtw_distance(self, ts_a, ts_b, d = lambda x,y: abs(x-y)):\n",
    "        \"\"\"Returns the DTW similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "        \n",
    "        d : DistanceMetric object (default = abs(x-y))\n",
    "            the distance measure used for A_i - B_j in the\n",
    "            DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "        return fastdtw(ts_a,ts_b)[0] \n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        ts_a, ts_b = np.array(ts_a), np.array(ts_b)\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "        cost = sys.maxsize * np.ones((M, N))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d(ts_a[0], ts_b[0])\n",
    "        for i in range(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d(ts_a[i], ts_b[0])\n",
    "\n",
    "        for j in range(1, N):\n",
    "            cost[0, j] = cost[0, j-1] + d(ts_a[0], ts_b[j])\n",
    "\n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in range(1, M):\n",
    "            for j in range(max(1, i - self.max_warping_window),\n",
    "                            min(N, i + self.max_warping_window)):\n",
    "                choices = cost[i - 1, j - 1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + d(ts_a[i], ts_b[j])\n",
    "\n",
    "        # Return DTW distance given window \n",
    "        return cost[-1, -1]\n",
    "    \n",
    "    def _dist_matrix(self, x, y):\n",
    "        \"\"\"Computes the M x N distance matrix between the training\n",
    "        dataset and testing dataset (y) using the DTW distance measure\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        y : array of shape [n_samples, n_timepoints]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Distance matrix between each item of x and y with\n",
    "            shape [training_n_samples, testing_n_samples]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the distance matrix        \n",
    "        dm_count = 0\n",
    "        \n",
    "        # Compute condensed distance matrix (upper triangle) of pairwise dtw distances\n",
    "        # when x and y are the same array\n",
    "        if(np.array_equal(x, y)):\n",
    "            x_s = np.shape(x)\n",
    "            dm = np.zeros((x_s[0] * (x_s[0] - 1)) // 2, dtype=np.double)\n",
    "            \n",
    "            p = ProgressBar(shape(dm)[0])\n",
    "            \n",
    "            for i in range(0, x_s[0] - 1):\n",
    "                for j in range(i + 1, x_s[0]):\n",
    "                    dm[dm_count] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                      y[j, ::self.subsample_step])\n",
    "                    \n",
    "                    dm_count += 1\n",
    "                    p.animate(dm_count)\n",
    "            \n",
    "            # Convert to squareform\n",
    "            dm = squareform(dm)\n",
    "            return dm\n",
    "        \n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        else:\n",
    "            x_s = np.shape(x)\n",
    "            y_s = np.shape(y)\n",
    "            dm = np.zeros((x_s[0], y_s[0])) \n",
    "            dm_size = x_s[0]*y_s[0]\n",
    "            \n",
    "            p = ProgressBar(dm_size)\n",
    "        \n",
    "            for i in range(0, x_s[0]):\n",
    "                for j in range(0, y_s[0]):\n",
    "                    dm[i, j] = self._dtw_distance(x[i, ::self.subsample_step],\n",
    "                                                  y[j, ::self.subsample_step])\n",
    "                    # Update progress bar\n",
    "                    dm_count += 1\n",
    "                    p.animate(dm_count)\n",
    "        \n",
    "            return dm\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the class labels or probability estimates for \n",
    "        the provided data\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "          x : array of shape [n_samples, n_timepoints]\n",
    "              Array containing the testing data set to be classified\n",
    "          \n",
    "        Returns\n",
    "        -------\n",
    "          2 arrays representing:\n",
    "              (1) the predicted class labels \n",
    "              (2) the knn label count probability\n",
    "        \"\"\"\n",
    "        \n",
    "        dm = self._dist_matrix(x, self.x)\n",
    "\n",
    "        # Identify the k nearest neighbors\n",
    "        knn_idx = dm.argsort()[:, :self.n_neighbors]\n",
    "\n",
    "        # Identify k nearest labels\n",
    "        knn_labels = self.l[knn_idx]\n",
    "        \n",
    "        # Model Label\n",
    "        mode_data = mode(knn_labels, axis=1)\n",
    "        mode_label = mode_data[0]\n",
    "        mode_proba = mode_data[1]/self.n_neighbors\n",
    "\n",
    "        return mode_label.ravel(), mode_proba.ravel()\n",
    "\n",
    "class ProgressBar:\n",
    "    \"\"\"This progress bar was taken from PYMC\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 40\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = self.animate_noipython\n",
    "\n",
    "    def animate_ipython(self, iter):\n",
    "        print('\\r', self)\n",
    "        sys.stdout.flush()\n",
    "        self.update_iteration(iter + 1)\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring the DTW distance\n",
    "The DTW distance between two sequences can be calculated using the `_dtw_distance()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "time = np.linspace(0,20,1000)\n",
    "amplitude_a = 5*np.sin(time)\n",
    "amplitude_b = 3*np.sin(time + 1)\n",
    "\n",
    "m = KnnDtw()\n",
    "distance = m._dtw_distance(amplitude_a, amplitude_b)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "_ = plt.plot(time, amplitude_a, label='A')\n",
    "_ = plt.plot(time, amplitude_b, label='B')\n",
    "_ = plt.title('DTW distance between A and B is %.2f' % distance)\n",
    "_ = plt.ylabel('Amplitude')\n",
    "_ = plt.xlabel('Time')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to compute the distance between each pair of two collections of inputs by using the `_dist_matrix()` method. These are the two methods which underpin the KnnDtw() classification algorithm. In the next section we will use the `fit()` and `predict()` method to train our classifier and predict class labels forunseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m._dist_matrix(np.random.random((4,50)), np.random.random((4,50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Activity Recognition Dataset\n",
    "The Human Activity Recognition Dataset (HAR) dataset is chosen to test the classification performance of DTW & KNN [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The experiments were carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (walking, walking upstairs, walking downstairs, sitting, standing and laying) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually.\n",
    "\n",
    "The remainder of this analysis uses a training and test dataset provided by the authors. They have combined the above timeseries signals and created a single timeseries feature vector. Unfortunately, their methodology is not described.\n",
    "\n",
    "####Import the HAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the HAR dataset\n",
    "x_train_file = open('../data/UCI-HAR-Dataset/train/X_train.txt', 'r')\n",
    "y_train_file = open('../data/UCI-HAR-Dataset/train/y_train.txt', 'r')\n",
    "\n",
    "x_test_file = open('../data/UCI-HAR-Dataset/test/X_test.txt', 'r')\n",
    "y_test_file = open('../data/UCI-HAR-Dataset/test/y_test.txt', 'r')\n",
    "\n",
    "# Create empty lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# Mapping table for classes\n",
    "labels = {1:'WALKING', 2:'WALKING UPSTAIRS', 3:'WALKING DOWNSTAIRS',\n",
    "          4:'SITTING', 5:'STANDING', 6:'LAYING'}\n",
    "\n",
    "# Loop through datasets\n",
    "for x in x_train_file:\n",
    "    x_train.append([float(ts) for ts in x.split()])\n",
    "    \n",
    "for y in y_train_file:\n",
    "    y_train.append(int(y.rstrip('\\n')))\n",
    "    \n",
    "for x in x_test_file:\n",
    "    x_test.append([float(ts) for ts in x.split()])\n",
    "    \n",
    "for y in y_test_file:\n",
    "    y_test.append(int(y.rstrip('\\n')))\n",
    "    \n",
    "# Convert to numpy for efficiency\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing sample observations from the HAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "colors = ['#D62728','#2C9F2C','#FD7F23','#1F77B4','#9467BD',\n",
    "          '#8C564A','#7F7F7F','#1FBECF','#E377C2','#BCBD27']\n",
    "\n",
    "for i, r in enumerate([0,27,65,100,145,172]):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.plot(x_train[r][:100], label=labels[y_train[r]], color=colors[i], linewidth=2)\n",
    "    plt.xlabel('Samples @50Hz')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance\n",
    "The performance of the KNN & DTW classifier is measured by comparing the class labels from the holdout dataset against predictions made by the classifier. The HAR dataset was pre-split into 70% training and 30% test. Cross validation was not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = KnnDtw(n_neighbors=1, max_warping_window=1)\n",
    "m.fit(x_train[::1000], y_train[::1000])\n",
    "label, proba = m.predict(x_test[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(label, y_test[::10],\n",
    "                            target_names=[l for l in labels.values()]))\n",
    "\n",
    "conf_mat = confusion_matrix(label, y_test[::10])\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "width = np.shape(conf_mat)[1]\n",
    "height = np.shape(conf_mat)[0]\n",
    "\n",
    "res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\n",
    "for i, row in enumerate(conf_mat):\n",
    "    for j, c in enumerate(row):\n",
    "        if c>0:\n",
    "            plt.text(j-.2, i+.1, c, fontsize=16)\n",
    "            \n",
    "cb = fig.colorbar(res)\n",
    "plt.title('Confusion Matrix')\n",
    "_ = plt.xticks(range(6), [l for l in labels.values()], rotation=90)\n",
    "_ = plt.yticks(range(6), [l for l in labels.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN & DTW classifier performed very well across each of the 6 classes. It even beats the best classification rate published by the authors of the HAR paper.\n",
    "\n",
    "However, this performance comes at a cost. The KNN algorithm is implemented using brute force. So, comparing 500 training observations to 500 test observations results in 250,000 DTW distances to be computed. And each (unconstrained) DTW distance calculation takes over 65K calculations each (256 x 256). This results in 16.3 billion calculations for just 500 x 500 observations.. Clearly not a scalable classification technique!\n",
    "\n",
    "Performance improvements can be achieved by reducing the `max_warping_window` parameter. However, these gains will not be sufficient to make KNN & DTW a viable classification technique for large or medium sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_taken = []\n",
    "windows = [1,2,5,10,50,100,500,1000,5000]\n",
    "\n",
    "for w in windows:\n",
    "    begin = time.time()\n",
    "    \n",
    "    t = KnnDtw(n_neighbors=1, max_warping_window=w)\n",
    "    t.fit(x_train[:20], y_train[:20])\n",
    "    label, proba = t.predict(x_test[:20])\n",
    "    \n",
    "    end = time.time()\n",
    "    time_taken.append(end - begin)\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "_ = plt.plot(windows, [t/400. for t in time_taken], lw=4)\n",
    "plt.title('DTW Execution Time with \\nvarying Max Warping Window')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.xlabel('Max Warping Window')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
